## **1. 背景**

我们经常讨论分布式系统的 CAP 理论，那么一定对 CAP 中的 C 有一定的了解，CAP 中 C 指的就是强一致性（**strong consistency** ），也就是线性一致性（**linearizability** ）。接下来我们准备分别撰写以下三部分内容，对线性一致性做一次深入的剖析。

1. 什么是线性一致性，线性一致性的实现和应用等
2. 线性一致性测试的理论：包含线性一致性的精确定义和线性一致性验证的算法介绍
3. 线性一致性测试的框架：主要介绍使用线性一致性测试框架 jepsen 的基本原理和使用

开篇首先先介绍线性一致性的定义、实现和应用等。

## **2. 定义**

## **2.1 什么是线性一致性？**

- **Linearizable semantics** （Linearizability）(each operation appears to execute instantaneously, exactly once, at some point between its invocation and its response)

- - 在一个线性一致性的系统里面，任何操作都可能在调用或者返回之间原子和瞬间执行
  - 线性一致性，Linearizability，也称为原子一致性（atomic consistency），强一致性（strong consistency）等
  - 也就是通常所说的 CAP 理论中的 C



比较模糊，下面慢慢解析，首先来看看，为什么需要线性一致性 ？还需要一个这么强的一致性 ？首先看一个简单的例子，如下图 [1]：



![img](https://pic4.zhimg.com/80/v2-df804a5179524594ba34fb9fb5991c33_720w.jpg)



1. Referee：更新比赛的最终结果，先 insert 到数据库 leader 副本，然后 Leader 再复制给两个 Follower 副本
2. Alice：从 Follower 1 中查到了最新的比赛分数
3. Bob：从 Follower 2 中确没查到最新的比赛分数，确显示比赛正在进行

如果 Alice 和 Bob 在同一个房间，各自盯着自己的手机观看比赛，Alice 刷新页面，返现 Germany 赢得比赛，然后告诉 Bob 比赛结果，而 Bob 刷新页面，确显示比赛正在进行，显然这个结果是让人不符合预期的，实际上它也不是符合线性一致性的。对 Bob 来说，他希望看到的结果应该是和 Alice 一样最新的比赛结果，而不是一个旧的结果。

上面的例子展示了非线性一致性的系统可能会返回一些不合情理的结果，这其实也反应了分布式系统中一个典型的问题 ？

> 分布式系统通常会是多个副本，那么多个副本复制会存在延迟 ，上图中每个副本就是一个数据库，但是看到了数据库复制中发生的一些时序问题 ，从而让外界看到多个副本的状态是不一致的，导致了一致性问题，



**一致性其实主要是描述了在故障和延迟的情况下副本间的状态协调的问题**

想象如果只有一个副本？或者对应用来说，看起来就像一个副本，那么是不是就很容易。

> 线性一致性的基本的想法是让一个系统看起来好像只有一个数据副本，而且所有的操作都是原子性的。有了这个保证，即使实际中可能有多个副本，应用也不需要担心它们。

## **2.2 什么样的系统是线性一致性的？**

系统需要加一些什么限制才能符合线性一致的语义呢 ？



![img](https://pic1.zhimg.com/80/v2-a8d8a539d4b262be9b962ca285076488_720w.jpg)



（图 9-2，横轴为时间，每个 Op 在时间轴上的起点表示表示 Op invocation 的时间戳，结束点表示 reponse）

从 client 端角度来看，线性一致性的系统需要如下约束：

- （1）单个 client Op 都是顺序的：如上图 9-2 [1] 每个 client 的每个 Op 都是在上一个 Op 返回之后，在执行下一个 Op，也就是同一个 client 的 Op 都是顺序的，没有并发
- （2）不同 client 的 Op 如果并发，则可能会返回旧值或新值：如上图 9-2，client B 的第一个 read 和 client C 的第1 个 write 是并发，那么 client B 有可能读到 read 0 或者 1 都是合法，因为它们是并发的，考虑到网络延迟，可能 read 先被执行，也可能 write 先被执行，或者相反（但是这条约束不足以完全描述线性一致性：如果与写入同时发生的读取可以返回旧值或新值，那么读者可能会在写入期间看到数值在旧值和新值之间来回翻转，如果 client B 的第1个 read 返回 0，第 2 个 read 返回 1，那么就出现新旧值的翻转了）
- （3）**任何一个读取返回新值后，所有后续读取（在相同或其他客户端上）也必须返回新值**：如下图 9-3 [1] 所示，client A 第 2 个 read、 client B 的第 1、2 个 read 和 client C 的第 1 个 write 是并发的，但是一旦 client A 的第 2 个 read 看到了这个最新值，那么 client B 的第 2 个 read 也必须看到这个最新的值



![img](https://pic2.zhimg.com/80/v2-5a509eb1013846047630a93d2fe22701_720w.jpg)

其实约束的第（3）条就是所谓的：

> **Linearizable semantics** （Linearizability）(each operation appears to execute instantaneously, exactly once, at some point between its invocation and its response).
> 在一个线性一致性的系统里面，任何操作都可能在**调用和返回之间原子和瞬间**执行。



也就是说，由于网络延迟的原因，所以在 client 端看来，一个 Op 是在 invocation 和 response 之间被执行，具体在这之间的某个时刻被执行，并不知道，但是如果这个 Op 在系统后端是在 invocation 和 response 之间的**特定时刻原子和瞬间**执行或者生效的，那么**任何一个读取返回新值后，所有后续读取（在相同或其他客户端上）也必须返回新值**，如下图 9-4，给出了每个 Op 原子或者瞬间执行的时间点就很清晰了



![img](https://pic4.zhimg.com/80/v2-179960ae9adf50c618c18f4217f2a40f_720w.jpg)

（注意：上图中 Client B read(x) => 2 不是线性一致性的）



**小结**

- 原子和瞬间的被执行，一旦执行成功，对**所有**的 client 可见
- 让多个副本，对应用来说看起来就像一个副本
- 线性一致性保证的是对单个对象的单个操作的保证瞬间或者原子的被执行，注意是事务 ACID 语义的区分，事务 ACID 语义是保证的一组操作（单个对象或者多个对象操作）
- 线性一致性并不局限在分布式系统，例如：在多核 CPU 中，如普通变量的 counter 就不是线性一致的，只有 atomic 变量才是线性一致的；还有很多如 queue、mutex 一般都必须满足线性化语义

## **3. 实现**

**如何实现线性一致性系统 ？**

现在最成熟的就是共识算法（Consensus Algorithm），例如 Paxos、Raft 等

**Raft**

这里将讨论 Raft 是否是线性一致的 ？以及其大致是如何实现线性一致的，这里不会过多的讨论 Raft 的细节，详细的细节可以参考 [2]。后续考虑给出单独的文章讨论 Raft 算法

- 线性一致性写

所有的 read/write 都会来到 Leader，write 会有 Op log Leader 被序列化，依次顺序往后 commit，并 apply 然后在返回，那么一旦一个 write 被 committed，那么其前面的 write 的 Op log 一定就被 committed 了。 所有的 write 都是有严格的顺序的，一旦被 committed 就可见了，所以 Raft 是线性一致性写，基本是没有什么问题的

- 线性一致性读

Raft 的 read 有多种实现：

1. Raft log read：每个 read 都有一个对应的 Op log，和 write 一样，都会走一遍一致性协议的流程，会在此 Read Op log 被 Apply 的时候读，那么这个 read Op log 之前的 write Op log 肯定也被 applied 了，那么一定能够被读取到，读到的也一定是最新的
2. ReadIndex：我们知道 Raft log read，会有 raft read log 的复制和提交的开销，所以出现了 ReadIndex，read 没有 Op log，但是需要额外的机制保证读到最新的，所以 read 发送给 Leader 的时候，1）它需要确认 read 返回的数据的那个点 ？必须返回最新 committed 的结果，但是一个节点刚当选 Leader 的时候并不知道最新的 committed index，这个时候需要提交一个 Noop log entry 来提交之前的 log entry，然后开始 Read；2）它需要确认当前的 Leader 是不是还是 Leader，因为可能因为网络分区，这个 Leader 已经被孤立了，所以 Leader 在返回 read 之前，先和 Replica-group 的其他成员发送 heartbeat 确定自己 Leader 的身份；通过上述两条保证读到最新被 committed 的数据
3. Lease read：主要是通过 lease 机制维护 Leader 的状态，来减少了 ReadIndex 每次 read 发送 heartheat 的开销，详细参考 [3]
4. Follower read：先去 Leader 查询最新的 committed index，然后拿着 committed Index 去 Follower read，从而保证能从 Follower 中读到最新的数据，当前 etcd 就实现了 Follower read

- 来看一个反例吧：

假设，对于实现 2. ReadIndex 方法实现，刚选出的 Leader 不通过提交 noop 来获取最新的 committed index，就返回读的话，会存在读取新旧 value 反转的情况：

1. 初始状态，三个副本 A，B，C，A 为 leader，B、C 为 Follower
2. client 发送`w1` 发送给 A，A committed w1，并且 apply 了，但是 B、C 还没有 committed `w1`
3. client 发送`r1` ，那么自然能够读到最新的写入 `w1`
4. Leader A 挂了，注意这个时候 B 和 C 还没有收到 A `w1` 的 committed 消息
5. B，C超时选举，假定 B 成为 Leader，显然 B 并不知道当前的最新 committed index 是多少
6. client 发送 `r2`，那么 B 就不一定能返回 `w1` 的结果了

上面的过程展示了 client 两次读 `r1` 和 `r2` 新旧反转的过程，是不满足线性一致的。所以对于刚起来的 Leader 来说，必须通过提交 noop 来提交已经被 commit，但是自己可能不知道的 log entry 来保证能读到最新的值

## **4. 应用**

线性一致性的系统有什么用呢 ？

- Leader 选举，一般通过分布式锁来实现，那么，必须保证这个分布式锁必须是满足线性一致性语义：也就是一旦某个 节点成为了 Leader，**其它节点的选举就必须返回失败**，这样的结果才是一致的，不然就会出现双主，就不合法了
- 对应用来说更简单：例如，设计和实现了一个块存储，不满足线性一致性，假定不能线性一致性读，那么在此块存储之上做其他的应用将很难，例如在这个块存储上面跑数据库，数据库就几乎不能做正确

## **5. 代价**

线性一致性，是一种强一致性，线性一致性的系统虽好，但是实现是有代价的

**原子数**

例如：在多核系统中，由于 CPU Cache 的原因，一个普通的变量，例如定义 `int count = 0`，可以对其执行 `get`（read）、`increment`（自增） 等操作，显然这个变量 `count` 不是满足线性一致性，例如 `increment` 不是原子的，`get` 可能返回一个旧的值等等。有两种方式让变量 `count` 满足线性一致性:

1. 加锁 `mutex`保护
2. 使用原子变量

那么自然，性能也就下降了。

**不总是需要线性一致**

所以在一些系统中，如果不需要线性一致性，那么可以做一些权衡，以 Figure 9-1 为例，用户可以接受一定的 read 延迟，只要不会出现新旧 value 反转即可，那么一个更弱的一致性 **单调读（Monotonic reads）**来避免时光倒流也是可以的，那么系统实现 read/write 就相对来说就简单些了：

以 read 为例，就不要向 Raft 一样总是去 Leader 读，只需要保证每次 read 都在相同的副本，就不会有任何问题，甚至对于新闻这种数据来说，偶尔出现一两次时光倒流也不会有什么不可饶恕的大问题 。

## **6. CAP**

既然聊到了 CAP 中的 C，那么也来聊聊 CAP 理论，这里以块存储场景为例，以常规的一致性思路来简单谈谈：

- CAP，P - 网络分区是一种错误，不是一个选项，一定会发生，所以，就是 CA 二选一
- 块存储，需要线性一致性（面向上层所有应用，没有tradeoff，必须线性一致性），所以就是 C 也没得选，就剩下 A 了
- 但是在网络分区的场景下，如果要保证线性一致性C，那么可用性A必然就会成为问题，尽管出现的概率极低（云盘厂商一般提供99.999% 5个9的可用性保障，这个对绝大多书应用场景而言，非常高了）



在确定CA的基础之上，系统设计应该关注一下2点，优秀的系统设计和实现会在一下2点上下足功夫，提供业界一流的云盘：

- 提高可用性

- - 在极端网络分区下，为保证线性一致性，只能牺牲可用性，网络分成三个区，{A}，{B}，{C}，那么就没办法提供服务了
  - 在一些非极端的网络分区情况下，例如 Raft 中在非对称和对称网络分区的情况下，通过 pre-vote 等提高可用性
  - 在大多数副本下线的情况下，而且场景允许，可以使用重置复制组提高可用性
  - 甚至通过一些结合客户端的机制，hinted handoff 重开复制组的机制尽最大努力保证读取和写入的可用性
  - 等等



- 提高性能

- - 线性一致性系统确实比较慢，这是一个事实，所以提高性能是如此的重要，Raft 性能优化，batch，pipeline，async commit & apply 各种提高性能的手段一起上 [4]，paxos 性能优化 [5]，paxos 变种 multi-paxos [6] 等等都是在为提高性能作出努力
  - 等等



**Notes**

作者：网易存储团队攻城狮 吴德妙

如有理解和描述上有疏漏或者错误的地方，欢迎共同交流；参考已经在参考文献中注明，但仍有可能有疏漏的地方，有任何侵权或者不明确的地方，欢迎指出，必定及时更正或者删除；文章供于学习交流，转载注明出处